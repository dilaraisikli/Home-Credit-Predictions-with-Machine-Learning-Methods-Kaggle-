{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_dataframes():\n",
    "    #read bureau, previous application, installments payments, application train and test dataframes  \n",
    "    #credit_card_balance = pd.read_csv(\"C:/Users/dilar/Documents/homecredit/credit_card_balance.csv\")\n",
    "    #pos_cash_balance = pd.read_csv(\"C:/Users/dilar/Documents/homecredit/POS_CASH_balance.csv\")\n",
    "    data_bureau = pd.read_csv(\"C:/Users/dilar/Documents/homecredit/bureau.csv\")\n",
    "    data_installments_payments = pd.read_csv(\"C:/Users/dilar/Documents/homecredit/installments_payments.csv\")\n",
    "    data_previous_application = pd.read_csv(\"C:/Users/dilar/Documents/homecredit/previous_application.csv\")\n",
    "    data_application_train = pd.read_csv(\"C:/Users/dilar/Documents/homecredit/application_train.csv\")\n",
    "    data_application_test =  pd.read_csv(\"C:/Users/dilar/Documents/homecredit/application_test.csv\")\n",
    "    return(data_bureau,data_installments_payments,data_previous_application,data_application_train,data_application_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
      "0         Cash loans           M            N               Y             0   \n",
      "1         Cash loans           F            N               N             0   \n",
      "2    Revolving loans           M            Y               Y             0   \n",
      "3         Cash loans           F            N               Y             0   \n",
      "4         Cash loans           M            N               Y             0   \n",
      "\n",
      "   AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE NAME_TYPE_SUITE  \\\n",
      "0          202500.0    406597.5      24700.5         351000.0   Unaccompanied   \n",
      "1          270000.0   1293502.5      35698.5        1129500.0          Family   \n",
      "2           67500.0    135000.0       6750.0         135000.0   Unaccompanied   \n",
      "3          135000.0    312682.5      29686.5         297000.0   Unaccompanied   \n",
      "4          121500.0    513000.0      21865.5         513000.0   Unaccompanied   \n",
      "\n",
      "   ... ('SK_DPD_DEF', 'max') ('NAME_CONTRACT_STATUS_y', '<lambda>')  \\\n",
      "0  ...                   NaN                                    NaN   \n",
      "1  ...                   NaN                                    NaN   \n",
      "2  ...                   NaN                                    NaN   \n",
      "3  ...                   0.0                                 Active   \n",
      "4  ...                   NaN                                    NaN   \n",
      "\n",
      "  ANN_RATIO_CRED DAYS_EMPLOYED_BIRTH  INCOME_ANN  GOODS_RATIO_CRED  RATEOFPAY  \\\n",
      "0      16.461104            0.067329    0.121978          1.158397   0.060749   \n",
      "1      36.234085            0.070862    0.132217          1.145199   0.027598   \n",
      "2      20.000000            0.011814    0.100000          1.000000   0.050000   \n",
      "3      10.532818            0.159905    0.219900          1.052803   0.094941   \n",
      "4      23.461618            0.152418    0.179963          1.000000   0.042623   \n",
      "\n",
      "   INCOME_CRED  CAR_RAT_EMP_RATIO  AMT_CRED_INCOME_RAT  \n",
      "0     0.498036                NaN             2.007889  \n",
      "1     0.208736                NaN             4.790750  \n",
      "2     0.500000          -0.115556             2.000000  \n",
      "3     0.431748                NaN             2.316167  \n",
      "4     0.236842                NaN             4.222222  \n",
      "\n",
      "[5 rows x 372 columns]\n"
     ]
    }
   ],
   "source": [
    "def Merge_Dataframes(data_bureau,data_installments_payments,data_previous_application,data_application_train,data_application_test):\n",
    "    #credit_card_balance_agg= credit_card_balance.groupby('SK_ID_CURR').agg({'MONTHS_BALANCE':['mean','std', 'min','max'],'AMT_BALANCE':['mean','std', 'min','max'],'AMT_CREDIT_LIMIT_ACTUAL':['mean','std', 'min','max'],'AMT_DRAWINGS_ATM_CURRENT':['mean','std', 'min','max'],'AMT_DRAWINGS_CURRENT':['mean','std', 'min','max'],'AMT_DRAWINGS_OTHER_CURRENT':['mean','std', 'min','max'],'AMT_DRAWINGS_POS_CURRENT':['mean','std', 'min','max'],'AMT_INST_MIN_REGULARITY':['mean','std', 'min','max'],'AMT_PAYMENT_CURRENT':['mean','std', 'min','max'],'AMT_PAYMENT_TOTAL_CURRENT':['mean','std', 'min','max'],'AMT_RECEIVABLE_PRINCIPAL':['mean','std', 'min','max'],'AMT_RECIVABLE':['mean','std', 'min','max'],'AMT_TOTAL_RECEIVABLE':['mean','std', 'min','max'],'CNT_DRAWINGS_ATM_CURRENT':['mean','std', 'min','max'],'CNT_DRAWINGS_CURRENT':['mean','std', 'min','max'],'CNT_DRAWINGS_OTHER_CURRENT':['mean','std', 'min','max'],'CNT_DRAWINGS_POS_CURRENT':['mean','std', 'min','max'],'CNT_INSTALMENT_MATURE_CUM':['mean','std', 'min','max'],'SK_DPD':['mean','std', 'min','max'],'SK_DPD_DEF':['mean','std', 'min','max'],'NAME_CONTRACT_STATUS':lambda x: x.mode()[0]})\n",
    "    #pos_cash_balance_agg = pos_cash_balance.groupby('SK_ID_CURR').agg({'MONTHS_BALANCE':['mean','std', 'min','max'],'CNT_INSTALMENT':['mean','std', 'min','max'],'CNT_INSTALMENT_FUTURE':['mean','std', 'min','max'],'SK_DPD':['mean','std', 'min','max'],'SK_DPD_DEF':['mean','std', 'min','max'],'NAME_CONTRACT_STATUS': lambda x: x.mode()[0]})\n",
    "    #find mean,std,min and max values of numerical columns and find mode for categorical columns\n",
    "    bureau_agg = data_bureau.groupby('SK_ID_CURR').agg({'DAYS_CREDIT':['mean','std', 'min','max'], 'CREDIT_DAY_OVERDUE':['mean','std', 'min','max'], 'DAYS_CREDIT_ENDDATE':['mean','std', 'min','max'], 'DAYS_ENDDATE_FACT':['mean','std', 'min','max'] , 'AMT_CREDIT_MAX_OVERDUE':['mean','std', 'min','max'] , 'AMT_CREDIT_SUM':['mean','std', 'min','max'], 'AMT_CREDIT_SUM_DEBT':['mean','std', 'min','max'],'AMT_CREDIT_SUM_LIMIT':['mean','std', 'min','max'], 'AMT_CREDIT_SUM_OVERDUE':['mean','std', 'min','max'], 'DAYS_CREDIT_UPDATE':['mean','std', 'min','max'], 'AMT_ANNUITY':['mean','std', 'min','max'], 'CREDIT_ACTIVE':lambda x: x.mode()[0] ,'CREDIT_CURRENCY':lambda x: x.mode()[0],'CNT_CREDIT_PROLONG': lambda x: x.mode()[0],'CREDIT_TYPE':lambda x: x.mode()[0]})\n",
    "    #find mean,std,min and max values of numerical columns and find mode for categorical columns\n",
    "    installments_payments_agg = data_installments_payments.groupby('SK_ID_CURR').agg({'NUM_INSTALMENT_VERSION':['mean','std', 'min','max'], 'NUM_INSTALMENT_NUMBER':['mean','std', 'min','max'] , 'DAYS_INSTALMENT':['mean','std', 'min','max'] , 'DAYS_ENTRY_PAYMENT':['mean','std', 'min','max'], 'AMT_INSTALMENT':['mean','std', 'min','max'],'AMT_PAYMENT':['mean','std', 'min','max']})\n",
    "    ##replace nan values for DAYS_EMPLOYED\n",
    "    data_previous_application['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    data_previous_application['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    data_previous_application['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    data_previous_application['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    data_previous_application['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    #find mean,std,min and max values of numerical columns and find mode for categorical columns\n",
    "    new_data_previous_application = data_previous_application.groupby('SK_ID_CURR').agg({'AMT_ANNUITY':['mean','std', 'min','max'],'AMT_APPLICATION':['mean','std', 'min','max'],'AMT_CREDIT':['mean','std', 'min','max'],'AMT_DOWN_PAYMENT':['mean','std', 'min','max'],'AMT_GOODS_PRICE':['mean','std', 'min','max'],'HOUR_APPR_PROCESS_START':['mean','std', 'min','max'],'NFLAG_LAST_APPL_IN_DAY':['mean','std', 'min','max'],'RATE_DOWN_PAYMENT':['mean','std', 'min','max'],'RATE_INTEREST_PRIMARY':['mean','std', 'min','max'],'RATE_INTEREST_PRIVILEGED':['mean','std', 'min','max'],'DAYS_DECISION':['mean','std', 'min','max'],'SELLERPLACE_AREA':['mean','std', 'min','max'],'CNT_PAYMENT':['mean','std', 'min','max'],'DAYS_FIRST_DRAWING':['mean','std', 'min','max'],'DAYS_FIRST_DUE':['mean','std', 'min','max'],'DAYS_LAST_DUE_1ST_VERSION':['mean','std', 'min','max'],'DAYS_LAST_DUE':['mean','std', 'min','max'],'DAYS_TERMINATION':['mean','std', 'min','max'],'NFLAG_INSURED_ON_APPROVAL':['mean','std', 'min','max'],'NAME_CONTRACT_TYPE':lambda x: x.mode()[0] ,'WEEKDAY_APPR_PROCESS_START':lambda x: x.mode()[0],'FLAG_LAST_APPL_PER_CONTRACT':lambda x: x.mode()[0],'NAME_CASH_LOAN_PURPOSE':lambda x: x.mode()[0] ,'NAME_CONTRACT_STATUS':lambda x: x.mode()[0] ,'NAME_PAYMENT_TYPE':lambda x: x.mode()[0] ,'CODE_REJECT_REASON':lambda x: x.mode()[0] ,'NAME_CLIENT_TYPE':lambda x: x.mode()[0],'NAME_GOODS_CATEGORY':lambda x: x.mode()[0],'NAME_PORTFOLIO':lambda x: x.mode()[0],'NAME_PRODUCT_TYPE': lambda x: x.mode()[0],'CHANNEL_TYPE':lambda x: x.mode()[0],'NAME_SELLER_INDUSTRY': lambda x: x.mode()[0] ,'NAME_YIELD_GROUP':lambda x: x.mode()[0],'PRODUCT_COMBINATION':lambda x: x.mode()[0]})\t\n",
    "    #merge all dataframes with train dataframe (considering SK_ID_CURR)\n",
    "    data_application_train1 = pd.merge(data_application_train, installments_payments_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    data_application_train2 = pd.merge(data_application_train1, bureau_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    data_application_train3 = pd.merge(data_application_train2, new_data_previous_application, on = 'SK_ID_CURR', how = 'left')\n",
    "    #data_application_train4 = pd.merge(data_application_train3, pos_cash_balance_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    #data_application_train5 = pd.merge(data_application_train3, credit_card_balance_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    #delete target and SK_ID_CURR from merged train dataframe\n",
    "    del data_application_train3 ['TARGET']\n",
    "    del data_application_train3['SK_ID_CURR']\n",
    "    #merge all dataframes with test dataframe (considering SK_ID_CURR)\n",
    "    data_application_test1 = pd.merge(data_application_test, installments_payments_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    data_application_test2 = pd.merge(data_application_test1, bureau_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    data_application_test3 = pd.merge(data_application_test2, new_data_previous_application, on = 'SK_ID_CURR', how = 'left')\n",
    "    #data_application_test4 = pd.merge(data_application_test3, pos_cash_balance_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    #data_application_test5 = pd.merge(data_application_test3, credit_card_balance_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    #delete SK_ID_CURR from merged test dataframe\n",
    "    del data_application_test3['SK_ID_CURR']\n",
    "    #concatenation for test and train dataframes\n",
    "    data_app_train_test=pd.concat([data_application_train3,data_application_test3])\n",
    "    #replace nan values for DAYS_EMPLOYED\n",
    "    data_app_train_test['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    #create new columns with using exist columns\n",
    "    data_app_train_test['ANN_RATIO_CRED'] = data_app_train_test['AMT_CREDIT'] / data_app_train_test['AMT_ANNUITY']\n",
    "    data_app_train_test['DAYS_EMPLOYED_BIRTH'] = data_app_train_test['DAYS_EMPLOYED'] / data_app_train_test['DAYS_BIRTH']\n",
    "    data_app_train_test['INCOME_ANN'] = data_app_train_test['AMT_ANNUITY'] / data_app_train_test['AMT_INCOME_TOTAL']\n",
    "    data_app_train_test['GOODS_RATIO_CRED'] = data_app_train_test['AMT_CREDIT'] / data_app_train_test['AMT_GOODS_PRICE']\n",
    "    data_app_train_test['RATEOFPAY'] = data_app_train_test['AMT_ANNUITY'] / data_app_train_test['AMT_CREDIT']\n",
    "    data_app_train_test['INCOME_CRED'] = data_app_train_test['AMT_INCOME_TOTAL'] / data_app_train_test['AMT_CREDIT']\n",
    "    data_app_train_test['CAR_RAT_EMP_RATIO'] = data_app_train_test['OWN_CAR_AGE'] / data_app_train_test['DAYS_EMPLOYED']\n",
    "    data_app_train_test['AMT_CRED_INCOME_RAT'] = data_app_train_test['AMT_CREDIT'] / data_app_train_test['AMT_INCOME_TOTAL']\n",
    "    #save dataframe to feature selection  \n",
    "    #data_app_train_test.to_csv(\"C:/Users/dilar/Documents/homecredit/data_app_train_test1.csv\", index = False)\n",
    "    return(data_app_train_test)\n",
    "Merge_Dataframes(Read_dataframes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  \\\n",
      "0              Cash loans           M            N               Y   \n",
      "1              Cash loans           F            N               N   \n",
      "2         Revolving loans           M            Y               Y   \n",
      "3              Cash loans           F            N               Y   \n",
      "4              Cash loans           M            N               Y   \n",
      "...                   ...         ...          ...             ...   \n",
      "356250         Cash loans           F            N               Y   \n",
      "356251         Cash loans           F            N               N   \n",
      "356252         Cash loans           F            Y               Y   \n",
      "356253         Cash loans           M            N               N   \n",
      "356254         Cash loans           F            Y               N   \n",
      "\n",
      "        CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_ANNUITY NAME_TYPE_SUITE  \\\n",
      "0                  0          202500.0      24700.5   Unaccompanied   \n",
      "1                  0          270000.0      35698.5          Family   \n",
      "2                  0           67500.0       6750.0   Unaccompanied   \n",
      "3                  0          135000.0      29686.5   Unaccompanied   \n",
      "4                  0          121500.0      21865.5   Unaccompanied   \n",
      "...              ...               ...          ...             ...   \n",
      "356250             0          121500.0      17473.5   Unaccompanied   \n",
      "356251             2          157500.0      31909.5   Unaccompanied   \n",
      "356252             1          202500.0      33205.5   Unaccompanied   \n",
      "356253             0          225000.0      25128.0          Family   \n",
      "356254             0          135000.0      24709.5   Unaccompanied   \n",
      "\n",
      "            NAME_INCOME_TYPE            NAME_EDUCATION_TYPE  ...  \\\n",
      "0                    Working  Secondary / secondary special  ...   \n",
      "1              State servant               Higher education  ...   \n",
      "2                    Working  Secondary / secondary special  ...   \n",
      "3                    Working  Secondary / secondary special  ...   \n",
      "4                    Working  Secondary / secondary special  ...   \n",
      "...                      ...                            ...  ...   \n",
      "356250               Working  Secondary / secondary special  ...   \n",
      "356251  Commercial associate  Secondary / secondary special  ...   \n",
      "356252  Commercial associate  Secondary / secondary special  ...   \n",
      "356253  Commercial associate               Higher education  ...   \n",
      "356254               Working  Secondary / secondary special  ...   \n",
      "\n",
      "       ('SK_DPD', 'min') ('SK_DPD_DEF', 'min')  \\\n",
      "0                    NaN                   NaN   \n",
      "1                    NaN                   NaN   \n",
      "2                    NaN                   NaN   \n",
      "3                    0.0                   0.0   \n",
      "4                    NaN                   NaN   \n",
      "...                  ...                   ...   \n",
      "356250               NaN                   NaN   \n",
      "356251               NaN                   NaN   \n",
      "356252               NaN                   NaN   \n",
      "356253               NaN                   NaN   \n",
      "356254               0.0                   0.0   \n",
      "\n",
      "        ('NAME_CONTRACT_STATUS_y', '<lambda>')  ANN_RATIO_CRED  INCOME_ANN  \\\n",
      "0                                          NaN       16.461104    0.121978   \n",
      "1                                          NaN       36.234085    0.132217   \n",
      "2                                          NaN       20.000000    0.100000   \n",
      "3                                       Active       10.532818    0.219900   \n",
      "4                                          NaN       23.461618    0.179963   \n",
      "...                                        ...             ...         ...   \n",
      "356250                                     NaN       23.610610    0.143815   \n",
      "356251                                     NaN       19.505570    0.202600   \n",
      "356252                                     NaN        9.486380    0.163978   \n",
      "356253                                     NaN       17.908309    0.111680   \n",
      "356254                                  Active       12.657804    0.183033   \n",
      "\n",
      "        GOODS_RATIO_CRED  RATEOFPAY  INCOME_CRED  CAR_RAT_EMP_RATIO  \\\n",
      "0               1.158397   0.060749     0.498036                NaN   \n",
      "1               1.145199   0.027598     0.208736                NaN   \n",
      "2               1.000000   0.050000     0.500000          -0.115556   \n",
      "3               1.052803   0.094941     0.431748                NaN   \n",
      "4               1.000000   0.042623     0.236842                NaN   \n",
      "...                  ...        ...          ...                ...   \n",
      "356250          1.528000   0.042354     0.294503                NaN   \n",
      "356251          1.257400   0.051267     0.253047                NaN   \n",
      "356252          1.000000   0.105414     0.642857          -0.001317   \n",
      "356253          1.000000   0.055840     0.500000                NaN   \n",
      "356254          1.158400   0.079003     0.431630          -0.034755   \n",
      "\n",
      "        AMT_CRED_INCOME_RAT  \n",
      "0                  2.007889  \n",
      "1                  4.790750  \n",
      "2                  2.000000  \n",
      "3                  2.316167  \n",
      "4                  4.222222  \n",
      "...                     ...  \n",
      "356250             3.395556  \n",
      "356251             3.951829  \n",
      "356252             1.555556  \n",
      "356253             2.000000  \n",
      "356254             2.316800  \n",
      "\n",
      "[356255 rows x 213 columns]>\n"
     ]
    }
   ],
   "source": [
    "def Correlation(cor_data,threshold):\n",
    "    #find correlation on them\n",
    "    correlations = cor_data.corr().apply(abs)\n",
    "    #create two lists to keep columns names which have correlation over 0.9\n",
    "    listI=[]\n",
    "    listJ=[]\n",
    "    for i in range(len(correlations)):\n",
    "        for j in range( len(correlations)):\n",
    "            if ((correlations.iloc[i,j]) > threshold) and (correlations.iloc[i,j] != 1):\n",
    "                if correlations.index[i] not in listI and correlations.index[i] not in listJ:\n",
    "                    listI.append(correlations.index[i])\n",
    "                if correlations.columns[j] not in listI and correlations.columns[j] not in listJ:\n",
    "                    listJ.append(correlations.columns[j])\n",
    "    #delete columns names which have correlation over 0.9\n",
    "    for col in listI:\n",
    "            del cor_data[col]\n",
    "    for col in listJ:\n",
    "            del cor_data[col]\n",
    "    \n",
    "    #save new dataframe after apply correlation\n",
    "    #cor_data.to_csv(\"C:/Users/dilar/Documents/homecredit/corr_data_app_train_test.csv\", index = False)\n",
    "    return(cor_data)\n",
    "Correlation(Merge_Dataframes(),0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  \\\n",
      "0              Cash loans           M            N               Y   \n",
      "1              Cash loans           F            N               N   \n",
      "2         Revolving loans           M            Y               Y   \n",
      "3              Cash loans           F            N               Y   \n",
      "4              Cash loans           M            N               Y   \n",
      "...                   ...         ...          ...             ...   \n",
      "356250         Cash loans           F            N               Y   \n",
      "356251         Cash loans           F            N               N   \n",
      "356252         Cash loans           F            Y               Y   \n",
      "356253         Cash loans           M            N               N   \n",
      "356254         Cash loans           F            Y               N   \n",
      "\n",
      "        CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_ANNUITY NAME_TYPE_SUITE  \\\n",
      "0                  0          202500.0      24700.5   Unaccompanied   \n",
      "1                  0          270000.0      35698.5          Family   \n",
      "2                  0           67500.0       6750.0   Unaccompanied   \n",
      "3                  0          135000.0      29686.5   Unaccompanied   \n",
      "4                  0          121500.0      21865.5   Unaccompanied   \n",
      "...              ...               ...          ...             ...   \n",
      "356250             0          121500.0      17473.5   Unaccompanied   \n",
      "356251             2          157500.0      31909.5   Unaccompanied   \n",
      "356252             1          202500.0      33205.5   Unaccompanied   \n",
      "356253             0          225000.0      25128.0          Family   \n",
      "356254             0          135000.0      24709.5   Unaccompanied   \n",
      "\n",
      "            NAME_INCOME_TYPE            NAME_EDUCATION_TYPE  ...  \\\n",
      "0                    Working  Secondary / secondary special  ...   \n",
      "1              State servant               Higher education  ...   \n",
      "2                    Working  Secondary / secondary special  ...   \n",
      "3                    Working  Secondary / secondary special  ...   \n",
      "4                    Working  Secondary / secondary special  ...   \n",
      "...                      ...                            ...  ...   \n",
      "356250               Working  Secondary / secondary special  ...   \n",
      "356251  Commercial associate  Secondary / secondary special  ...   \n",
      "356252  Commercial associate  Secondary / secondary special  ...   \n",
      "356253  Commercial associate               Higher education  ...   \n",
      "356254               Working  Secondary / secondary special  ...   \n",
      "\n",
      "       ('CHANNEL_TYPE', '<lambda>') ('NAME_SELLER_INDUSTRY', '<lambda>')  \\\n",
      "0                             Stone                      Auto technology   \n",
      "1                      Country-wide                 Consumer electronics   \n",
      "2                  Regional / Local                         Connectivity   \n",
      "3           Credit and cash offices                                  XNA   \n",
      "4                      Country-wide                 Consumer electronics   \n",
      "...                             ...                                  ...   \n",
      "356250      Credit and cash offices                                  XNA   \n",
      "356251                 Country-wide                 Consumer electronics   \n",
      "356252                 Country-wide                             Clothing   \n",
      "356253      Credit and cash offices                         Connectivity   \n",
      "356254      Credit and cash offices                                  XNA   \n",
      "\n",
      "        ('NAME_YIELD_GROUP', '<lambda>')  ('PRODUCT_COMBINATION', '<lambda>')  \\\n",
      "0                             low_normal              POS other with interest   \n",
      "1                                 middle                     Cash X-Sell: low   \n",
      "2                                 middle          POS mobile without interest   \n",
      "3                                    XNA                                 Cash   \n",
      "4                                   high                  Cash X-Sell: middle   \n",
      "...                                  ...                                  ...   \n",
      "356250                        low_normal                     Cash Street: low   \n",
      "356251                              high                    Cash X-Sell: high   \n",
      "356252                        low_normal           POS industry with interest   \n",
      "356253                              high             POS mobile with interest   \n",
      "356254                               XNA                                 Cash   \n",
      "\n",
      "        ANN_RATIO_CRED  INCOME_ANN  GOODS_RATIO_CRED  RATEOFPAY  INCOME_CRED  \\\n",
      "0            16.461104    0.121978          1.158397   0.060749     0.498036   \n",
      "1            36.234085    0.132217          1.145199   0.027598     0.208736   \n",
      "2            20.000000    0.100000          1.000000   0.050000     0.500000   \n",
      "3            10.532818    0.219900          1.052803   0.094941     0.431748   \n",
      "4            23.461618    0.179963          1.000000   0.042623     0.236842   \n",
      "...                ...         ...               ...        ...          ...   \n",
      "356250       23.610610    0.143815          1.528000   0.042354     0.294503   \n",
      "356251       19.505570    0.202600          1.257400   0.051267     0.253047   \n",
      "356252        9.486380    0.163978          1.000000   0.105414     0.642857   \n",
      "356253       17.908309    0.111680          1.000000   0.055840     0.500000   \n",
      "356254       12.657804    0.183033          1.158400   0.079003     0.431630   \n",
      "\n",
      "        AMT_CRED_INCOME_RAT  \n",
      "0                  2.007889  \n",
      "1                  4.790750  \n",
      "2                  2.000000  \n",
      "3                  2.316167  \n",
      "4                  4.222222  \n",
      "...                     ...  \n",
      "356250             3.395556  \n",
      "356251             3.951829  \n",
      "356252             1.555556  \n",
      "356253             2.000000  \n",
      "356254             2.316800  \n",
      "\n",
      "[356255 rows x 167 columns]>\n"
     ]
    }
   ],
   "source": [
    "def Find_Missing_Values(miss_data,missing_percent):\n",
    "    listMiss=[]\n",
    "    deleteMissColNames=[]\n",
    "    #find missing data in dataframe\n",
    "    mis_val = miss_data.isnull().sum()\n",
    "    #find percentage of missing data in dataframe\n",
    "    mis_val_percent = 100 * miss_data.isnull().sum() / len(miss_data)\n",
    "    #calculate percantage of missing value for each columns and keep them which have over 0.60\n",
    "    for i in range(len(mis_val)):\n",
    "        if (mis_val[i] != 0 and mis_val_percent[i]>missing_percent ):\n",
    "            listMiss.append([miss_data.columns[i],mis_val[i],mis_val_percent[i]])\n",
    "            deleteMissColNames.append(miss_data.columns[i])\n",
    "    #delete columns which have missing values over 0.60\n",
    "    for col in deleteMissColNames:\n",
    "            del miss_data[col]\n",
    "     #save dataframe without missing value       \n",
    "    #miss_data.to_csv(\"C:/Users/dilar/Documents/homecredit/extant_corr_data_app_train_test.csv\", index = False)\n",
    "    return(miss_data)\n",
    "Find_Missing_Values(Correlation(),0.60)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NAME_CONTRACT_TYPE_Cash loans  NAME_CONTRACT_TYPE_Revolving loans  \\\n",
      "0                              1                                   0   \n",
      "1                              1                                   0   \n",
      "2                              0                                   1   \n",
      "3                              1                                   0   \n",
      "4                              1                                   0   \n",
      "\n",
      "   CODE_GENDER_F  CODE_GENDER_M  CODE_GENDER_XNA  FLAG_OWN_CAR_N  \\\n",
      "0              0              1                0               1   \n",
      "1              1              0                0               1   \n",
      "2              0              1                0               0   \n",
      "3              1              0                0               1   \n",
      "4              0              1                0               1   \n",
      "\n",
      "   FLAG_OWN_CAR_Y  FLAG_OWN_REALTY_N  FLAG_OWN_REALTY_Y  \\\n",
      "0               0                  0                  1   \n",
      "1               0                  1                  0   \n",
      "2               1                  0                  1   \n",
      "3               0                  0                  1   \n",
      "4               0                  0                  1   \n",
      "\n",
      "   NAME_TYPE_SUITE_Children  ...  ('NFLAG_INSURED_ON_APPROVAL', 'mean')  \\\n",
      "0                         0  ...                               0.000000   \n",
      "1                         0  ...                               0.666667   \n",
      "2                         0  ...                               0.000000   \n",
      "3                         0  ...                               0.000000   \n",
      "4                         0  ...                               0.600000   \n",
      "\n",
      "   ('NFLAG_INSURED_ON_APPROVAL', 'std')  ('NFLAG_INSURED_ON_APPROVAL', 'min')  \\\n",
      "0                                   NaN                                   0.0   \n",
      "1                              0.816497                                   0.0   \n",
      "2                                   NaN                                   0.0   \n",
      "3                              0.000000                                   0.0   \n",
      "4                              0.774597                                   0.0   \n",
      "\n",
      "   ('NFLAG_INSURED_ON_APPROVAL', 'max')  ANN_RATIO_CRED  INCOME_ANN  \\\n",
      "0                                   0.0        0.226047    0.060141   \n",
      "1                                   1.0        0.756604    0.065198   \n",
      "2                                   0.0        0.321005    0.049285   \n",
      "3                                   0.0        0.066977    0.108509   \n",
      "4                                   1.0        0.413888    0.088782   \n",
      "\n",
      "   GOODS_RATIO_CRED  RATEOFPAY  INCOME_CRED  AMT_CRED_INCOME_RAT  \n",
      "0          0.172376   0.377861     0.002338             0.023640  \n",
      "1          0.170120   0.053985     0.000947             0.056483  \n",
      "2          0.145299   0.272843     0.002347             0.023547  \n",
      "3          0.154325   0.711908     0.002019             0.027278  \n",
      "4          0.145299   0.200770     0.001082             0.049774  \n",
      "\n",
      "[5 rows x 423 columns]\n"
     ]
    }
   ],
   "source": [
    "def Normalization_Encoding(data_nor_encod):\n",
    "    #find numerical columns in dataframe\n",
    "    num_cols = list(data_nor_encod._get_numeric_data().columns)\n",
    "    #create list with all columns name\n",
    "    cols=list(data_nor_encod.columns)\n",
    "    #remove numeric columns to find categorical columns\n",
    "    for i in range(len(num_cols)):\n",
    "        if num_cols[i] in cols:\n",
    "            cols.remove(num_cols[i])\n",
    "    #create 2 dataframes for categorical and numerical \n",
    "    numericDf = data_nor_encod[num_cols].copy()\n",
    "    catecDf = data_nor_encod[cols].copy()\n",
    "    \n",
    "    #use minmaxscaler function to make normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    numericDf[num_cols] = scaler.fit_transform(numericDf)\n",
    "    #use dummies function to one hot encoding\n",
    "    encoding_catecDf=pd.get_dummies(catecDf)\n",
    "    #merge categorical and numerical columns dataframe\n",
    "    result = pd.concat([encoding_catecDf, numericDf], axis=1, sort=False)\n",
    "    #save dataframe after make normalization and encoding \n",
    "    #result.to_csv(\"C:/Users/dilar/Documents/homecredit/encod_nor_extant_corr_data_app_train_test.csv\", index = False)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.84533\ttrain's binary_logloss: 0.499948\tvalid's auc: 0.774324\tvalid's binary_logloss: 0.533139\n",
      "[400]\ttrain's auc: 0.891503\ttrain's binary_logloss: 0.445858\tvalid's auc: 0.77582\tvalid's binary_logloss: 0.497899\n",
      "Early stopping, best iteration is:\n",
      "[338]\ttrain's auc: 0.878742\ttrain's binary_logloss: 0.461325\tvalid's auc: 0.776186\tvalid's binary_logloss: 0.507648\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.842644\ttrain's binary_logloss: 0.503712\tvalid's auc: 0.779948\tvalid's binary_logloss: 0.52916\n",
      "[400]\ttrain's auc: 0.889962\ttrain's binary_logloss: 0.44879\tvalid's auc: 0.780954\tvalid's binary_logloss: 0.493851\n",
      "Early stopping, best iteration is:\n",
      "[368]\ttrain's auc: 0.883644\ttrain's binary_logloss: 0.456649\tvalid's auc: 0.7812\tvalid's binary_logloss: 0.499092\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.790799\ttrain's binary_logloss: 0.561038\tvalid's auc: 0.768143\tvalid's binary_logloss: 0.570282\n",
      "[400]\ttrain's auc: 0.815539\ttrain's binary_logloss: 0.531801\tvalid's auc: 0.778586\tvalid's binary_logloss: 0.547762\n",
      "[600]\ttrain's auc: 0.833146\ttrain's binary_logloss: 0.512201\tvalid's auc: 0.781193\tvalid's binary_logloss: 0.535091\n",
      "[800]\ttrain's auc: 0.847998\ttrain's binary_logloss: 0.495837\tvalid's auc: 0.782071\tvalid's binary_logloss: 0.524677\n",
      "[1000]\ttrain's auc: 0.861151\ttrain's binary_logloss: 0.481162\tvalid's auc: 0.782628\tvalid's binary_logloss: 0.515339\n",
      "[1200]\ttrain's auc: 0.873123\ttrain's binary_logloss: 0.467525\tvalid's auc: 0.78268\tvalid's binary_logloss: 0.506631\n",
      "Early stopping, best iteration is:\n",
      "[1174]\ttrain's auc: 0.871668\ttrain's binary_logloss: 0.469233\tvalid's auc: 0.782771\tvalid's binary_logloss: 0.507753\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.790275\ttrain's binary_logloss: 0.561721\tvalid's auc: 0.770196\tvalid's binary_logloss: 0.570451\n",
      "[400]\ttrain's auc: 0.814439\ttrain's binary_logloss: 0.532889\tvalid's auc: 0.780493\tvalid's binary_logloss: 0.548303\n",
      "[600]\ttrain's auc: 0.831783\ttrain's binary_logloss: 0.513598\tvalid's auc: 0.783542\tvalid's binary_logloss: 0.5355\n",
      "[800]\ttrain's auc: 0.846566\ttrain's binary_logloss: 0.497301\tvalid's auc: 0.784662\tvalid's binary_logloss: 0.525106\n",
      "[1000]\ttrain's auc: 0.859958\ttrain's binary_logloss: 0.482346\tvalid's auc: 0.785407\tvalid's binary_logloss: 0.515528\n",
      "[1200]\ttrain's auc: 0.872106\ttrain's binary_logloss: 0.468616\tvalid's auc: 0.785659\tvalid's binary_logloss: 0.506613\n",
      "[1400]\ttrain's auc: 0.883221\ttrain's binary_logloss: 0.455606\tvalid's auc: 0.785954\tvalid's binary_logloss: 0.498287\n",
      "Early stopping, best iteration is:\n",
      "[1399]\ttrain's auc: 0.883175\ttrain's binary_logloss: 0.455665\tvalid's auc: 0.785955\tvalid's binary_logloss: 0.498332\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.790204\ttrain's binary_logloss: 0.561748\tvalid's auc: 0.770577\tvalid's binary_logloss: 0.571991\n",
      "[400]\ttrain's auc: 0.814593\ttrain's binary_logloss: 0.53288\tvalid's auc: 0.780279\tvalid's binary_logloss: 0.550193\n",
      "[600]\ttrain's auc: 0.831895\ttrain's binary_logloss: 0.513559\tvalid's auc: 0.783293\tvalid's binary_logloss: 0.537414\n",
      "[800]\ttrain's auc: 0.846661\ttrain's binary_logloss: 0.497412\tvalid's auc: 0.784439\tvalid's binary_logloss: 0.526752\n",
      "[1000]\ttrain's auc: 0.859907\ttrain's binary_logloss: 0.482663\tvalid's auc: 0.785186\tvalid's binary_logloss: 0.517346\n",
      "[1200]\ttrain's auc: 0.872002\ttrain's binary_logloss: 0.469055\tvalid's auc: 0.785283\tvalid's binary_logloss: 0.508363\n",
      "Early stopping, best iteration is:\n",
      "[1103]\ttrain's auc: 0.866235\ttrain's binary_logloss: 0.475576\tvalid's auc: 0.785325\tvalid's binary_logloss: 0.512379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.791731\ttrain's binary_logloss: 0.560202\tvalid's auc: 0.763368\tvalid's binary_logloss: 0.568796\n",
      "[400]\ttrain's auc: 0.816058\ttrain's binary_logloss: 0.53122\tvalid's auc: 0.77451\tvalid's binary_logloss: 0.54645\n",
      "[600]\ttrain's auc: 0.833411\ttrain's binary_logloss: 0.511737\tvalid's auc: 0.778453\tvalid's binary_logloss: 0.533257\n",
      "[800]\ttrain's auc: 0.848151\ttrain's binary_logloss: 0.495315\tvalid's auc: 0.779807\tvalid's binary_logloss: 0.522838\n",
      "[1000]\ttrain's auc: 0.861086\ttrain's binary_logloss: 0.480842\tvalid's auc: 0.78008\tvalid's binary_logloss: 0.51371\n",
      "[1200]\ttrain's auc: 0.873177\ttrain's binary_logloss: 0.466956\tvalid's auc: 0.780317\tvalid's binary_logloss: 0.504778\n",
      "Early stopping, best iteration is:\n",
      "[1204]\ttrain's auc: 0.87342\ttrain's binary_logloss: 0.466679\tvalid's auc: 0.780347\tvalid's binary_logloss: 0.504598\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.790528\ttrain's binary_logloss: 0.56133\tvalid's auc: 0.76988\tvalid's binary_logloss: 0.567961\n",
      "[400]\ttrain's auc: 0.815122\ttrain's binary_logloss: 0.532181\tvalid's auc: 0.779479\tvalid's binary_logloss: 0.545941\n",
      "[600]\ttrain's auc: 0.832673\ttrain's binary_logloss: 0.512705\tvalid's auc: 0.782493\tvalid's binary_logloss: 0.532751\n",
      "[800]\ttrain's auc: 0.847594\ttrain's binary_logloss: 0.49623\tvalid's auc: 0.783239\tvalid's binary_logloss: 0.522356\n",
      "[1000]\ttrain's auc: 0.860736\ttrain's binary_logloss: 0.481478\tvalid's auc: 0.78365\tvalid's binary_logloss: 0.513142\n",
      "Early stopping, best iteration is:\n",
      "[1058]\ttrain's auc: 0.864387\ttrain's binary_logloss: 0.477375\tvalid's auc: 0.783764\tvalid's binary_logloss: 0.510467\n",
      "valid_scr: 0.7836324452649801\n"
     ]
    }
   ],
   "source": [
    "def Feature_Importance(application_train,application_test,encod_nor_extant_corr_data_app_train_test):\n",
    "    # cross validation with k = 2 to have more precision on the features scores\n",
    "    n_splits = 2 \n",
    "    k_fold = KFold(n_splits = n_splits, shuffle = True)\n",
    "    zeroNum = []\n",
    "    \n",
    "    sep_train = encod_nor_extant_corr_data_app_train_test.iloc[:307511]\n",
    "    sep_test = encod_nor_extant_corr_data_app_train_test.iloc[307511]\n",
    "    ID = application_test['SK_ID_CURR']\n",
    "    target= application_train['TARGET']\n",
    "    \n",
    "    train = sep_train.to_numpy()\n",
    "    target = target.to_numpy()\n",
    "    test = sep_test.to_numpy()\n",
    "    \n",
    "    # array to save features scores\n",
    "    feature_importances = np.zeros(train.shape[1])\n",
    "    \n",
    "    # cross validation loop\n",
    "    for train_indices, valid_indices in k_fold.split(train):\n",
    "        train_features, train_labels = train[train_indices], target[train_indices]\n",
    "        valid_features, valid_labels = train[valid_indices], target[valid_indices]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', boosting_type='goss',\n",
    "                                       class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                       reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1 )\n",
    "        \n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], \n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        # at each step of the cross validation the feature importances are saved in this array; we take the average of this on the 2 folds\n",
    "        feature_importances += model.feature_importances_/ n_splits\n",
    "        # clean memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # delete feature with 0 importance\n",
    "    for i in range(len(feature_importances)):\n",
    "        if feature_importances[i] == 0:\n",
    "            zeroNum.append(i)\n",
    "    \n",
    "    train = np.delete(train, zeroNum,1)\n",
    "    test = np.delete(test, zeroNum, 1)\n",
    "    return(train,test,target,ID) \n",
    "\n",
    "\n",
    "def Learning_Model(train,test,target,ID):\n",
    "    # LGBM model with 5-fold cross validation\n",
    "    n_splits = 5\n",
    "    k_fold = KFold(n_splits = n_splits, shuffle = True)\n",
    "    \n",
    "    test_predictions = np.zeros(target.shape[0])\n",
    "    feature_importances = np.zeros(train.shape[1])\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    \n",
    "    # arrays for validation and train AUC scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # model loop\n",
    "    for train_indices, valid_indices in k_fold.split(train):\n",
    "        train_features, train_labels = train[train_indices], target[train_indices]\n",
    "        valid_features, valid_labels = train[valid_indices], target[valid_indices]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', boosting_type='goss',\n",
    "                                       class_weight = 'balanced', learning_rate = 0.02, \n",
    "                                       reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1 )\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], \n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # save best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        # feature imporatance scores\n",
    "        feature_importances += model.feature_importances_/ n_splits\n",
    "        # apply model on test set, the final result is given by the average scores on the 5 folds\n",
    "        test_predictions += model.predict_proba(test, num_iteration = best_iteration)[:, 1] / n_splits\n",
    "        \n",
    "        # train and validation scores\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "    \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # clean memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "    print('valid_scr:', sum(valid_scores)/5)\n",
    "    # create submission file\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': list(ID), 'TARGET': test_predictions})\n",
    "    submission.to_csv(\"C:/Users/dilar/Documents/homecredit/submission.csv\", index=False)\n",
    "\n",
    "#read application train and test dataframes  \n",
    "data_application_train = pd.read_csv(\"C:/Users/dilar/Documents/homecredit/application_train.csv\")\n",
    "data_application_test =  pd.read_csv(\"C:/Users/dilar/Documents/homecredit/application_test.csv\")\n",
    "\n",
    "Learning_Model(Feature_Importance(data_application_train,data_application_test,Normalization_Encoding()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
